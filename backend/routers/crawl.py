from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session
from pydantic import BaseModel
from typing import List
from database import get_db
from models import CrawlConfig
from datetime import datetime
import requests
from bs4 import BeautifulSoup
from sqlalchemy import text
from services.training_service import train_from_web_pages
from routers.logs_ws import broadcast_log
from schemas.crawl import CrawlConfigSchema 
from services.spider_crawler import crawl_and_download_files
from services.training_service import train_from_uploaded_files
import urllib.parse
from urllib.parse import urlparse, unquote


router = APIRouter(prefix="/crawl", tags=["crawl"])

# âœ… Schema Pydantic Ä‘á»ƒ nháº­n request
class CrawlIn(BaseModel):
    url: str
    selector: str
    label: str = ""

def is_valid_url(url):
    parsed = urlparse(url)
    return bool(parsed.scheme and parsed.netloc)

# ================================
# ğŸ” Láº¥y danh sÃ¡ch cáº¥u hÃ¬nh crawl
# ================================
@router.get("", response_model=List[CrawlIn])
def get_all_crawls(db: Session = Depends(get_db)):
    return db.query(CrawlConfig).all()

# ================================
# â• ThÃªm cáº¥u hÃ¬nh crawl
# ================================
@router.post("")
def add_crawl(config: CrawlConfigSchema, db: Session = Depends(get_db)):
    try:
        crawl = CrawlConfig(
            url=config.url,
            depth=config.depth,
            use_browser=config.use_browser
        )
        db.add(crawl)
        db.commit()
        return {"message": "âœ… ÄÃ£ thÃªm crawl config!"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Lá»—i khi thÃªm config: {e}")


# ================================
# âŒ XoÃ¡ cáº¥u hÃ¬nh crawl
# ================================
@router.delete("/{id}")
def delete_crawl(id: int, db: Session = Depends(get_db)):
    crawl = db.query(CrawlConfig).filter(CrawlConfig.id == id).first()
    if not crawl:
        raise HTTPException(status_code=404, detail="KhÃ´ng tÃ¬m tháº¥y cáº¥u hÃ¬nh")
    db.delete(crawl)
    db.commit()
    return {"message": "ğŸ—‘ï¸ ÄÃ£ xoÃ¡ config crawl"}

# ================================
# ğŸš€ Cháº¡y crawl táº¥t cáº£ config
# ================================
@router.post("/run")
async def run_crawl_all(db: Session = Depends(get_db)):
    configs = db.query(CrawlConfig).all()
    count = 0
    for conf in configs:
        try:
            r = requests.get(conf.url, timeout=10)
            soup = BeautifulSoup(r.text, "html.parser")
            items = soup.select(conf.selector)
            for it in items:
                link = it.select_one("a")
                content = it.text.strip()
                url = link["href"].strip() if link else None
                if not url:
                    continue
                db.execute(
                    text("""
                        INSERT INTO web_pages (url, content)
                        VALUES (:url, :content)
                        ON CONFLICT (url) DO NOTHING
                    """), {"url": url, "content": content}
                )
                count += 1
            db.commit()
            await broadcast_log(f"ğŸŒ Crawled {conf.label or conf.url} - ThÃªm {count} báº£n ghi.")
        except Exception as e:
            await broadcast_log(f"âš ï¸ Lá»—i crawl {conf.url}: {e}")
    return {"message": f"âœ… ÄÃ£ crawl {len(configs)} config, thÃªm {count} báº£n ghi."}

# ================================
# âš¡ Crawl & huáº¥n luyá»‡n tá»©c thÃ¬
# ================================
@router.post("/instant")
async def instant_crawl(payload: dict, db: Session = Depends(get_db)):
    url = payload.get("url")
    url = urllib.parse.unquote(url)

    url = unquote(payload.get("url", "").strip())
    if not url:
        raise HTTPException(status_code=400, detail="Thiáº¿u URL")

    parsed = urlparse(url)
    if not parsed.scheme or not parsed.netloc:
        raise HTTPException(status_code=400, detail="âŒ URL khÃ´ng há»£p lá»‡")

    if not url or not url.strip():
        raise HTTPException(status_code=400, detail="Thiáº¿u URL")

    try:
        await broadcast_log(f"ğŸ•¸ï¸ Äang báº¯t Ä‘áº§u crawl spider tá»«: {url}")
        await crawl_and_download_files(url, depth=2)  # ğŸ‘ˆ Crawl Ä‘á»‡ quy trong domain, Ä‘á»™ sÃ¢u 2

        await broadcast_log(f"ğŸ“¥ ÄÃ£ táº£i xong tÃ i liá»‡u tá»« {url}")
        await broadcast_log(f"ğŸ§  Äang huáº¥n luyá»‡n...")

        await train_from_uploaded_files()

        return {"message": f"âœ… Crawl & train tá»« {url} hoÃ n táº¥t!"}

    except Exception as e:
        await broadcast_log(f"âŒ Lá»—i huáº¥n luyá»‡n: {e}")
        raise HTTPException(status_code=500, detail=f"Lá»—i khi crawl: {e}")