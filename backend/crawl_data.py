import requests
import os
import time
import psycopg2
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse

# ==========================
# üöÄ C·∫•u h√¨nh database
# ==========================
DB_CONFIG = {
    "dbname": "chatbot_db",
    "user": "chatbot_user",
    "password": "secretpassword",
    "host": "oanhbihi-postgres",  # Ho·∫∑c "localhost" n·∫øu ch·∫°y ngo√†i Docker
    "port": "5432"
}

conn = psycopg2.connect(**DB_CONFIG)
cursor = conn.cursor()

# ==========================
# üöÄ T·∫°o b·∫£ng web_pages ƒë·ªÉ l∆∞u d·ªØ li·ªáu crawl
# ==========================
CREATE_TABLE_QUERY = """
        CREATE TABLE IF NOT EXISTS web_pages (
            id SERIAL PRIMARY KEY,
            url TEXT UNIQUE NOT NULL,
            title TEXT,
            content TEXT,
            file_path TEXT,
            last_crawled TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            embedding vector(1536)
        );
"""
cursor.execute(CREATE_TABLE_QUERY)
conn.commit()
print("‚úÖ B·∫£ng `web_pages` ƒë√£ ƒë∆∞·ª£c t·∫°o ho·∫∑c ƒë√£ t·ªìn t·∫°i.")

# ==========================
# üöÄ C·∫•u h√¨nh Crawler
# ==========================
BASE_URL = "https://fta.dainam.edu.vn"
VISITED_URLS = set()
DOWNLOAD_FOLDER = "downloads"

# T·∫°o th∆∞ m·ª•c l∆∞u file
if not os.path.exists(DOWNLOAD_FOLDER):
    os.makedirs(DOWNLOAD_FOLDER)

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36"
}

# ==========================
# üöÄ H√†m t·∫£i file n·∫øu c√≥
# ==========================
def download_file(file_url):
    file_name = file_url.split("/")[-1]
    file_path = os.path.join(DOWNLOAD_FOLDER, file_name)
    
    try:
        response = requests.get(file_url, headers=HEADERS, stream=True)
        if response.status_code == 200:
            with open(file_path, "wb") as file:
                for chunk in response.iter_content(chunk_size=1024):
                    file.write(chunk)
            print(f"üìÇ ƒê√£ t·∫£i: {file_name}")
            return file_path
    except Exception as e:
        print(f"‚ùå L·ªói t·∫£i file {file_url}: {e}")
    
    return None

# ==========================
# üöÄ H√†m Crawl trang web
# ==========================
def crawl_page(url):
    if url in VISITED_URLS:
        return
    VISITED_URLS.add(url)

    try:
        response = requests.get(url, headers=HEADERS, timeout=10)
        if response.status_code != 200:
            print(f"‚ö†Ô∏è L·ªói {response.status_code} - B·ªè qua: {url}")
            return

        soup = BeautifulSoup(response.text, "html.parser")
        title = soup.title.string.strip() if soup.title else "Kh√¥ng c√≥ ti√™u ƒë·ªÅ"
        
        # L·∫•y n·ªôi dung ch√≠nh
        content = " ".join([p.text.strip() for p in soup.find_all("p")])

        # L∆∞u v√†o database
        cursor.execute(
            "INSERT INTO web_pages (url, title, content) VALUES (%s, %s, %s) ON CONFLICT (url) DO NOTHING;",
            (url, title, content)
        )
        conn.commit()
        print(f"‚úÖ ƒê√£ l∆∞u: {url}")

        # T·∫£i xu·ªëng c√°c file PDF, DOCX
        for link in soup.find_all("a", href=True):
            file_url = urljoin(url, link["href"])
            if file_url.endswith((".pdf", ".docx", ".xls", ".xlsx")):
                file_path = download_file(file_url)
                if file_path:
                    cursor.execute(
                        "UPDATE web_pages SET file_path = %s WHERE url = %s;",
                        (file_path, url)
                    )
                    conn.commit()

        # Crawl ti·∫øp c√°c link trong trang (ch·ªâ link n·ªôi b·ªô)
        for a_tag in soup.find_all("a", href=True):
            next_url = urljoin(url, a_tag["href"])
            if urlparse(next_url).netloc == urlparse(BASE_URL).netloc:
                crawl_page(next_url)

    except Exception as e:
        print(f"‚ùå L·ªói crawl {url}: {e}")

# ==========================
# üöÄ Ch·∫°y crawler
# ==========================
if __name__ == "__main__":
    print(f"üöÄ B·∫Øt ƒë·∫ßu crawl t·ª´ {BASE_URL}")
    crawl_page(BASE_URL)

    cursor.close()
    conn.close()
    print("üöÄ Ho√†n th√†nh qu√° tr√¨nh crawl v√† l∆∞u d·ªØ li·ªáu!")
